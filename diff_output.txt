diff --git a/doctr/datasets/generator/base.py b/doctr/datasets/generator/base.py
index 4046559..6412587 100644
--- a/doctr/datasets/generator/base.py
+++ b/doctr/datasets/generator/base.py
@@ -3,7 +3,7 @@
 # This program is licensed under the Apache License 2.0.
 # See LICENSE or go to <https://opensource.org/licenses/Apache-2.0> for full license details.
 
-import random
+import random, os, time
 from typing import Any, Callable, List, Optional, Tuple, Union
 
 from PIL import Image, ImageDraw
@@ -109,6 +109,8 @@ class _WordGenerator(AbstractDataset):
         max_chars: int,
         num_samples: int,
         cache_samples: bool = False,
+        words_txt_path: Optional[str] = None,
+        save_samples_root: Optional[str] = None,
         font_family: Optional[Union[str, List[str]]] = None,
         img_transforms: Optional[Callable[[Any], Any]] = None,
         sample_transforms: Optional[Callable[[Any, Any], Tuple[Any, Any]]] = None,
@@ -126,6 +128,25 @@ class _WordGenerator(AbstractDataset):
                     raise ValueError(f"unable to locate font: {font}")
         self.img_transforms = img_transforms
         self.sample_transforms = sample_transforms
+        self.words_txt_path = words_txt_path
+        self.words_txt_file_contents = None
+
+        self.save_samples_root = save_samples_root
+        if self.save_samples_root:
+            os.makedirs(os.path.join(self.save_samples_root,"images"))
+
+        if words_txt_path:
+            try:
+                with open(words_txt_path, "r", encoding="utf-8") as file:
+                    self.words_txt_file_contents = file.readlines()
+            except FileNotFoundError:
+                self.words_txt_file_contents = None
+                print(f"File not found: {word_txt_path}")
+            except Exception as e:
+                self.words_txt_file_contents = None
+                print(f"An error occurred: {e}")
+
+        #print("self.words_txt_file_contents",type(self.words_txt_file_contents),len(self.words_txt_file_contents),self.words_txt_file_contents[10000:10001])
 
         self._data: List[Image.Image] = []
         if cache_samples:
@@ -133,8 +154,25 @@ class _WordGenerator(AbstractDataset):
             self._data = [
                 (synthesize_text_img(text, font_family=random.choice(self.font_family)), text) for text in _words
             ]
+        self.counter = 0
 
     def _generate_string(self, min_chars: int, max_chars: int) -> str:
+        
+        if self.words_txt_file_contents:
+            #print("self.words_txt_file_contents",self.words_txt_file_contents)
+            #words_list = open(self.words_txt_path, "r", encoding="utf-8").readlines()
+            #print(random.choice(words_list),type(random.choice(words_list)))
+            word=""
+            while (1):
+                word=random.choice(self.words_txt_file_contents).rstrip("\n")
+                if len(word)>12:
+                    continue
+                else:
+                    break
+            
+            return word
+
+            
         num_chars = random.randint(min_chars, max_chars)
         return "".join(random.choice(self.vocab) for _ in range(num_chars))
 
@@ -148,6 +186,10 @@ class _WordGenerator(AbstractDataset):
         else:
             target = self._generate_string(*self.wordlen_range)
             pil_img = synthesize_text_img(target, font_family=random.choice(self.font_family))
+        if self.save_samples_root:
+            img_name = str(self.counter)+".png"
+            pil_img.save(os.path.join(self.save_samples_root,"images",img_name))
+            self.counter += 1
         img = tensor_from_pil(pil_img)
 
         return img, target
diff --git a/doctr/datasets/vocabs.py b/doctr/datasets/vocabs.py
index 1a88a6e..3ae0d59 100644
--- a/doctr/datasets/vocabs.py
+++ b/doctr/datasets/vocabs.py
@@ -20,12 +20,37 @@ VOCABS: Dict[str, str] = {
     "hindi_digits": "Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©",
     "arabic_diacritics": "ŸãŸåŸçŸéŸèŸêŸëŸí",
     "arabic_punctuation": "ÿüÿõ¬´¬ª‚Äî",
+    "sanskrit_unicode": "‡§Ä‡§Å‡§Ç‡§É‡§Ñ‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§ã‡§å‡§ç‡§é‡§è‡§ê‡§ë‡§í‡§ì‡§î‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§©‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§±‡§≤‡§≥‡§¥‡§µ‡§∂‡§∑‡§∏‡§π‡§∫‡§ª‡§º‡§Ω‡§æ‡§ø‡•Ä‡•Å‡•Ç‡•É‡•Ñ‡•Ö‡•Ü‡•á‡•à‡•â‡•ä‡•ã‡•å‡•ç‡•é‡•è‡•ê‡•ë‡•í‡•ì‡•ò‡•ô‡•ö‡•õ‡•ú‡•ù‡•û‡•ü‡•†‡•¢‡•£‡•§‡••‡•¶‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø‡•∞‡•±‡•≤‡•≥‡•¥‡•µ‡•∂‡•∑‡•∏‡•π‡•∫‡•ª‡•º‡•Ω‡•æ‡•ø",
+    "sanskrit_numerals": "‡•ß‡•®‡•©‡•™‡•´‡•¨‡•≠‡•Æ‡•Ø‡•¶",
+    "sanskrit_alphabets":"‡§Ö‡§Ü‡§á‡§à‡§â‡§ä‡§è‡§ê‡§ì‡§î‡§Ö‡§Ç‡§Ö‡§Å‡§Ö‡§É‡§Ü‡§É‡§Ü‡§Ç‡§Ü‡§Å‡§á‡§É‡§á‡§Ç‡§á‡§Å‡§à‡§É‡§à‡§Ç‡§â‡§É‡§â‡§Å‡§â‡§Ç‡§ä‡§É‡§ä‡§Å‡§ä‡§Ç‡§è‡§É‡§è‡§Å‡§è‡§Ç‡§ê‡§É‡§ê‡§Ç‡§ì‡§É‡§ì‡§Ç‡§î‡§É‡§î‡§Ç‡§ã‡§ï‡§ñ‡§ó‡§ò‡§ô‡§ö‡§õ‡§ú‡§ù‡§û‡§ü‡§†‡§°‡§¢‡§£‡§§‡§•‡§¶‡§ß‡§®‡§™‡§´‡§¨‡§≠‡§Æ‡§Ø‡§∞‡§≤‡§µ‡§∏‡§∂‡§∑‡§π‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡§ú‡•ç‡§û‡§ï‡§æ‡§ï‡§ø‡§ï‡•Ä‡§ï‡•Å‡§ï‡•Ç‡§ï‡•ç‡§∞‡§∞‡•ç‡§ï‡§ï‡•É‡§ï‡•ã‡§ï‡•å‡§ï‡•á‡§ï‡•à‡§ï‡§Ç‡§ï‡§º‡§ï‡•â‡§ï‡§É‡§ï‡§ñ‡§æ‡§ñ‡§ø‡§ñ‡•Ä‡§ñ‡•Å‡§ñ‡•Ç‡§ñ‡•á‡§ñ‡•à‡§ñ‡•ç‡§∞‡§∞‡•ç‡§ñ‡§ñ‡§º‡§ñ‡•â‡§ñ‡§Ç‡§ñ‡•É‡§ñ‡•ã‡§ñ‡•å‡§ñ‡§É‡§ó‡§æ‡§ó‡§ø‡§ó‡•Ä‡§ó‡•Å‡§ó‡•Ç‡§ó‡•á‡§ó‡•à‡§ó‡•ã‡§ó‡•å‡§ó‡•É‡§ó‡•ç‡§∞‡§∞‡•ç‡§ó‡§ó‡•â‡§ó‡§º‡§ó‡§Ç‡§ó‡§É‡§ò‡•å‡§ò‡•à‡§ò‡§æ‡§ò‡•Ä‡§ò‡•Ç‡§ò‡§º‡§ò‡•â‡§ò‡•É‡§ò‡•ã‡§ò‡•á‡§ò‡§ø‡§ò‡•Å‡§ò‡§Ç‡§ò‡•Ö‡§ò‡§É‡§ò‡§Å‡§ò‡§ô‡•É‡§ô‡•å‡§ô‡•à‡§ô‡§æ‡§ô‡•Ä‡§ô‡•Ç‡§ô‡§ô‡•â‡§ô‡•ã‡§ô‡•á‡§ô‡•ç‡§ô‡§ø‡§ô‡•Å‡§ô‡§Ç‡§ô‡•ç‡§∞‡§ô‡•Ö‡§ô‡§Å‡§ö‡•É‡§ö‡•å‡§ö‡•à‡§ö‡§æ‡§ö‡•Ä‡§ö‡•Ç‡§ö‡§º‡§ö‡•â‡§ö‡•ã‡§ö‡•á‡§ö‡§ø‡§ö‡•Å‡§ö‡§Ç‡§ö‡•ç‡§∞‡§ö‡•Ö‡§ö‡§Å‡§∞‡•ç‡§ö‡§ö‡§É‡§õ‡•É‡§õ‡•å‡§õ‡•à‡§õ‡§æ‡§õ‡•Ä‡§õ‡•Ç‡§õ‡§º‡§õ‡•â‡§õ‡•ã‡§õ‡•á‡§õ‡§ø‡§õ‡•Å‡§õ‡§Ç‡§õ‡•Ö‡§õ‡•ç‡§∞‡§∞‡•ç‡§õ‡§õ‡§Å‡§õ‡§É‡§ú‡•É‡§ú‡•å‡§ú‡•à‡§ú‡§æ‡§ú‡•Ä‡§ú‡•Ç‡§ú‡§º‡§ú‡•â‡§ú‡•ã‡§ú‡•á‡§ú‡§ø‡§ú‡•Å‡§ú‡§Ç‡§ú‡•Ö‡§ú‡•ç‡§∞‡§∞‡•ç‡§ú‡§ú‡§Å‡§ú‡§É‡§ù‡•É‡§ù‡•å‡§ù‡•à‡§ù‡§æ‡§ù‡•Ä‡§ù‡•Ç‡§ù‡§º‡§ù‡•â‡§ù‡•ã‡§ù‡•á‡§ù‡§ø‡§ù‡•Å‡§ù‡§Ç‡§ù‡•Ö‡§ù‡•ç‡§∞‡§ù‡§É‡§∞‡•ç‡§ù‡§ù‡§Å‡§û‡§É‡§û‡•â‡§û‡§Ç‡§û‡§Å‡§ü‡•É‡§ü‡•å‡§ü‡•à‡§ü‡§æ‡§ü‡•Ä‡§ü‡•Ç‡§ü‡§º‡§ü‡•â‡§ü‡•ã‡§ü‡•á‡§ü‡§ø‡§ü‡•Å‡§ü‡§Ç‡§ü‡•Ö‡§ü‡•ç‡§∞‡§∞‡•ç‡§ü‡§ü‡§É‡§ü‡§Å‡§†‡•É‡§†‡•å‡§†‡•à‡§†‡§æ‡§†‡•Ä‡§†‡•Ç‡§†‡§º‡§†‡•â‡§†‡•ã‡§†‡•á‡§†‡§ø‡§†‡•Å‡§†‡§Ç‡§†‡•Ö‡§†‡•ç‡§∞‡§†‡§É‡§∞‡•ç‡§†‡§†‡§Å‡§°‡•É‡§°‡•å‡§°‡•à‡§°‡§æ‡§°‡•Ä‡§°‡•Ç‡§°‡•â‡§°‡•ã‡§°‡•á‡§°‡§ø‡§°‡•Å‡§°‡§Ç‡§°‡•Ö‡§°‡•ç‡§∞‡§°‡§É‡§∞‡•ç‡§°‡§°‡§Å‡§¢‡•É‡§¢‡•å‡§¢‡•à‡§¢‡§æ‡§¢‡•Ä‡§¢‡•Ç‡§¢‡§º‡§¢‡•â‡§¢‡•ã‡§¢‡•á‡§¢‡§ø‡§¢‡•Å‡§¢‡§Ç‡§¢‡•Ö‡§¢‡•ç‡§∞‡§¢‡§É‡§¢‡§Å‡§£‡•É‡§£‡•å‡§£‡•à‡§£‡§æ‡§£‡•Ä‡§£‡•Ç‡§£‡•â‡§£‡•ã‡§£‡•á‡§£‡§ø‡§£‡•Å‡§£‡§Ç‡§£‡•Ö‡§£‡•ç‡§∞‡§£‡§É‡§£‡§Å‡§∞‡•ç‡§£‡§§‡•É‡§§‡•å‡§§‡•à‡§§‡§æ‡§§‡•Ä‡§§‡•Ç‡§§‡§º‡§§‡•â‡§§‡•ã‡§§‡•á‡§§‡§ø‡§§‡•Å‡§§‡§Ç‡§§‡•Ö‡§§‡•ç‡§∞‡§§‡§É‡§§‡§§‡§Å‡§∞‡•ç‡§§‡§•‡•É‡§•‡•å‡§•‡•à‡§•‡§æ‡§•‡•Ä‡§•‡•Ç‡§•‡§º‡§•‡•â‡§•‡•ã‡§•‡•á‡§•‡§ø‡§•‡•Å‡§•‡§Ç‡§•‡•Ö‡§•‡•ç‡§∞‡§•‡§É‡§•‡§Å‡§∞‡•ç‡§•‡§¶‡•É‡§¶‡•å‡§¶‡•à‡§¶‡§æ‡§¶‡•Ä‡§¶‡•Ç‡§¶‡§º‡§¶‡•â‡§¶‡•ã‡§¶‡•á‡§¶‡§ø‡§¶‡•Å‡§¶‡§Ç‡§¶‡•Ö‡§¶‡•ç‡§∞‡§¶‡§É‡§¶‡§Å‡§∞‡•ç‡§¶‡§ß‡•É‡§ß‡•å‡§ß‡•à‡§ß‡§æ‡§ß‡•Ä‡§ß‡•Ç‡§ß‡§º‡§ß‡•â‡§ß‡•ã‡§ß‡•á‡§ß‡§ø‡§ß‡•Å‡§ß‡§Ç‡§ß‡•Ö‡§ß‡•ç‡§∞‡§ß‡§É‡§ß‡§Å‡§∞‡•ç‡§ß‡§®‡•å‡§®‡•à‡§®‡§æ‡§®‡•Ä‡§®‡•Ç‡§®‡•É‡§®‡•ã‡§®‡•á‡§®‡§ø‡§®‡•Å‡§®‡§Ç‡§®‡•Ö‡§®‡•ç‡§∞‡§®‡§É‡§®‡§Å‡§®‡•â‡§®‡§º‡§™‡•É‡§™‡•å‡§™‡•à‡§™‡§æ‡§™‡•Ä‡§™‡•Ç‡§™‡§º‡§™‡•â‡§™‡•ã‡§™‡•á‡§™‡§ø‡§™‡•Å‡§™‡§Ç‡§™‡•Ö‡§™‡•ç‡§∞‡§™‡§É‡§∞‡•ç‡§™‡§™‡§Å‡§´‡•É‡§´‡•å‡§´‡•à‡§´‡§æ‡§´‡•Ä‡§´‡•Ç‡§´‡§º‡§´‡•â‡§´‡•ã‡§´‡•á‡§´‡§ø‡§´‡•Å‡§´‡§Ç‡§´‡•Ö‡§´‡•ç‡§∞‡§´‡§É‡§´‡§Å‡§Å‡§∞‡•ç‡§´‡§¨‡•É‡§¨‡•å‡§¨‡§æ‡§¨‡•Ä‡§¨‡•Ç‡§¨‡§º‡§¨‡•â‡§¨‡•ã‡§¨‡•á‡§¨‡§ø‡§¨‡•Å‡§¨‡§Ç‡§¨‡•Ö‡§¨‡•ç‡§∞‡§¨‡§É‡§∞‡•ç‡§¨‡§¨‡§Å‡§≠‡•É‡§≠‡•å‡§≠‡•à‡§≠‡§æ‡§≠‡•Ä‡§≠‡•Ç‡§≠‡§º‡§≠‡•â‡§≠‡•ã‡§≠‡•á‡§≠‡§ø‡§≠‡•Å‡§≠‡§Ç‡§≠‡•Ö‡§≠‡•ç‡§∞‡§≠‡§É‡§≠‡§Å‡§∞‡•ç‡§≠‡§Æ‡•É‡§Æ‡•å‡§Æ‡•à‡§Æ‡§æ‡§Æ‡•Ä‡§Æ‡•Ç‡§Æ‡§º‡§Æ‡•â‡§Æ‡•ã‡§Æ‡•á‡§Æ‡§ø‡§Æ‡•Å‡§Æ‡§Ç‡§Æ‡•Ö‡§Æ‡•ç‡§∞‡§Æ‡§É‡§Æ‡§Å‡§∞‡•ç‡§Æ‡§Ø‡•É‡§Ø‡•å‡§Ø‡•à‡§Ø‡§æ‡§Ø‡•Ä‡§Ø‡•Ç‡§Ø‡§º‡§Ø‡•â‡§Ø‡•ã‡§Ø‡•á‡§Ø‡§ø‡§Ø‡•Å‡§Ø‡§Ç‡§Ø‡•Ö‡§Ø‡•ç‡§∞‡§Ø‡§É‡§Ø‡§Å‡§∞‡•ç‡§Ø‡§∞‡•É‡§∞‡•å‡§∞‡•à‡§∞‡§æ‡§∞‡•Ä‡§∞‡•Ç‡§∞‡§º‡§∞‡•â‡§∞‡•ã‡§∞‡•á‡§∞‡§ø‡§∞‡•Å‡§∞‡§Ç‡§∞‡•Ö‡§∞‡•ç‡§∞‡§∞‡§É‡§∞‡§Å‡§≤‡•É‡§≤‡•å‡§≤‡•à‡§≤‡§æ‡§≤‡•Ä‡§≤‡•Ç‡§≤‡§º‡§≤‡•â‡§≤‡•ã‡§≤‡•á‡§≤‡§ø‡§≤‡•Å‡§≤‡§Ç‡§≤‡•Ö‡§≤‡•ç‡§∞‡§≤‡§É‡§≤‡§Å‡§∞‡•ç‡§≤‡§µ‡•É‡§µ‡•å‡§µ‡•à‡§µ‡§æ‡§µ‡•Ä‡§µ‡•Ç‡§µ‡§º‡§µ‡•â‡§µ‡•ã‡§µ‡•á‡§µ‡§ø‡§µ‡•Å‡§µ‡§Ç‡§µ‡•Ö‡§µ‡•ç‡§∞‡§µ‡§É‡§µ‡§Å‡§∞‡•ç‡§µ‡§∏‡•É‡§∏‡•å‡§∏‡•à‡§∏‡§æ‡§∏‡•Ä‡§∏‡•Ç‡§∏‡§º‡§∏‡•â‡§∏‡•ã‡§∏‡•á‡§∏‡§ø‡§∏‡•Å‡§∏‡§∏‡§Ç‡§∏‡•Ö‡§∏‡•ç‡§∞‡§∏‡§É‡§∏‡§Å‡§∞‡•ç‡§∏‡§∂‡•É‡§∂‡•å‡§∂‡•à‡§∂‡§æ‡§∂‡•Ä‡§∂‡•Ç‡§∂‡§º‡§∂‡•â‡§∂‡•ã‡§∂‡•á‡§∂‡§ø‡§∂‡•Å‡§∂‡§Ç‡§∂‡•Ö‡§∂‡•ç‡§∞‡§∂‡§É‡§∂‡§Å‡§∞‡•ç‡§∂‡§∑‡•É‡§∑‡•å‡§∑‡•à‡§∑‡§æ‡§∑‡•Ä‡§∑‡•Ç‡§∑‡§º‡§∑‡•â‡§∑‡•ã‡§∑‡•á‡§∑‡§ø‡§∑‡•Å‡§∑‡§Ç‡§∑‡•Ö‡§∑‡•ç‡§∞‡§∑‡§É‡§∞‡•ç‡§∑‡§∑‡§Å‡§π‡•É‡§π‡•å‡§π‡•à‡§π‡§æ‡§π‡•Ä‡§π‡•Ç‡§π‡§º‡§π‡•â‡§π‡•ã‡§π‡•á‡§π‡§ø‡§π‡•Å‡§π‡§Ç‡§π‡•Ö‡§π‡•ç‡§∞‡§π‡§É‡§π‡§Å‡§∞‡•ç‡§π‡§∂‡•ç‡§∞‡•É‡§∂‡•ç‡§∞‡•å‡§∂‡•ç‡§∞‡•à‡§∂‡•ç‡§∞‡§æ‡§∂‡•ç‡§∞‡•Ä‡§∂‡•ç‡§∞‡•Ç‡§∂‡•ç‡§∞‡•â‡§∂‡•ç‡§∞‡•ã‡§∂‡•ç‡§∞‡•á‡§∂‡•ç‡§∞‡§ø‡§∂‡•ç‡§∞‡•Å‡§∂‡•ç‡§∞‡§Ç‡§∂‡•ç‡§∞‡•Ö‡§∂‡•ç‡§∞‡§É‡§∂‡•ç‡§∞‡§Å‡§∞‡•ç‡§∂‡•ç‡§∞‡§ï‡•ç‡§∑‡•É‡§ï‡•ç‡§∑‡•å‡§ï‡•ç‡§∑‡•à‡§ï‡•ç‡§∑‡§æ‡§ï‡•ç‡§∑‡•Ä‡§ï‡•ç‡§∑‡•Ç‡§ï‡•ç‡§∑‡§º‡§ï‡•ç‡§∑‡•â‡§ï‡•ç‡§∑‡•ã‡§ï‡•ç‡§∑‡•á‡§ï‡•ç‡§∑‡§ø‡§ï‡•ç‡§∑‡•Å‡§ï‡•ç‡§∑‡§Ç‡§ï‡•ç‡§∑‡•Ö‡§ï‡•ç‡§∑‡•ç‡§∞‡§ï‡•ç‡§∑‡§É‡§ï‡•ç‡§∑‡§Å‡§∞‡•ç‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡•É‡§§‡•ç‡§∞‡•å‡§§‡•ç‡§∞‡•à‡§§‡•ç‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä‡§§‡•ç‡§∞‡•Ç‡§§‡•ç‡§∞‡§º‡§§‡•ç‡§∞‡•â‡§§‡•ç‡§∞‡•ã‡§§‡•ç‡§∞‡•á‡§§‡•ç‡§∞‡§ø‡§§‡•ç‡§∞‡•Å‡§§‡•ç‡§∞‡§Ç‡§§‡•ç‡§∞‡•Ö‡§§‡•ç‡§∞‡§É‡§§‡•ç‡§∞‡§Å‡§∞‡•ç‡§§‡•ç‡§∞‡§≥‡§É",
+    
+    "sanskrit_transliterated": "'(-0123456789:A[abcdeghijklmnoprstuvy|√±ƒÅƒìƒ´≈ç≈õ≈´ÃÑÃêÃ•‡•§‡••·∏ç·∏•·πÅ·πÉ·πÖ·πá·πõ·π£·π≠" #.\u200c\u200d",
+
 }
 
-VOCABS["latin"] = VOCABS["digits"] + VOCABS["ascii_letters"] + VOCABS["punctuation"]
+VOCABS['bengali'] = '‡¶∂‡ß¶‡ßÇ‡ßÉ‡ß∞‡¶ú‡¶Ü‡¶î‡¶Ö‡¶ä‡¶ø‡ßù‡¶ñ‡ßµ‡¶™‡¶¢‡¶á‡ß≥‡¶´‡¶Ω‡ß™‡¶≤‡ßá‡¶ê‡¶Ø‡¶É‡¶à‡¶†‡ßÅ‡¶ß‡ßú‡ß≤‡ßÑ‡¶•‡¶≠‡¶ü‡¶Å‡¶ã‡ß±‡¶∞‡¶°‡ß¢‡¶õ‡ß¥‡¶ô‡¶ì‡¶ò‡¶∏‡ßß‡ßπ‡¶£‡¶ó‡ß∑‡ß©‡¶§‡ßÆ‡¶π‡ß≠‡ßã‡¶∑‡ßé‡ß∂‡¶ï‡¶®‡ß¨‡¶ö‡¶Æ‡ßà‡¶æ‡¶º‡ßÄ‡ß†‡¶ù‡¶è‡ßª‡¶¨‡ßØ‡ßü‡¶â‡ßå‡¶û‡ß∫‡ß®‡¶Ç‡ß£‡¶¶‡ß´‡ßç‡ßó-‡•§'
+VOCABS['gujarati'] = '‡´Æ‡™≤‡™î‡´®‡™∏‡™ñ‡™æ‡™ë‡™à‡™ã‡´ê‡™ì‡™µ‡´Ñ‡´¶‡™º‡™Å‡™®‡™û‡™ä‡´´‡´Ä‡™∂‡™´‡™£‡´¨‡´≠‡™¨‡´ß‡™∞‡™≥‡´å‡´Å‡™†‡™ê‡™â‡™∑‡™™‡´á‡™á‡™Ö‡´É‡™ù‡™ú‡´â‡™ï‡´±‡´Ø‡™ó‡™ç‡™¶‡´ã‡´™‡´Ö‡™è‡™Ç‡™π‡™°‡™ò‡´©‡´Ç‡™õ‡™ô‡™É‡™Ω‡™ü‡™§‡™ß‡™ø‡´à‡™Ø‡™¢‡´ç‡™Ü‡™Æ‡™•‡™ö‡™≠-'
+VOCABS['gurumukhi'] = '‡©ö‡®µ‡®®‡®Å‡®∞‡®ä‡®ñ‡®Ç‡®Ü‡®ú‡©à‡®≤‡©¥‡®£‡©ß‡®õ‡®≠‡®´‡©Æ‡©Ø‡®ö‡®î‡©Ä‡®Ø‡®π‡®≥‡®û‡©©‡©ú‡©û‡©Å‡®Æ‡©´‡®§‡©á‡®¶‡®∏‡®º‡®ü‡©∞‡©≠‡®ì‡®Ö‡®É‡®°‡®æ‡®â‡®†‡©±‡®à‡©¶‡©µ‡©ô‡®è‡®ï‡®•‡©¨‡®ß‡©≤‡©ë‡®ù‡®ø‡©®‡®ê‡®¨‡®™‡®ò‡®∂‡®ô‡©å‡©õ‡©ã‡®ó‡©ç‡©≥‡®á‡©™‡©Ç‡®¢-‡•§'
+VOCABS['kannada'] = '‡≤ö‡≥ï‡≤í‡≤â‡≥ñ‡≤Ç‡≤≤‡≤æ‡≤ù‡≤ü‡≥Ü‡≤Ö‡≥¨‡≥á‡≥®‡≤¨‡≤°‡≤µ‡≤ú‡≤¢‡≤û‡≤î‡≤è‡≤ß‡≤∂‡≤≠‡≤§‡≤≥‡≥Ä‡≤ï‡≤ê‡≤à‡≤†‡≤™‡≥´‡≤£‡≥Æ‡≥û‡≤Ü‡≤Ø‡≥Å‡≤ó‡≥¢‡≤ã‡≤¶‡≤ò‡≥Ç‡≥ç‡≥à‡≥¶‡≤ì‡≤±‡≤É‡≤π‡≥Ø‡≥ã‡≤Æ‡≥≠‡≥†‡≤•‡≤ñ‡≤´‡≤á‡≤∞‡≥™‡≤õ‡≤ô‡≥£‡≤ø‡≥©‡≥å‡≥Ñ‡≤∑‡≤å‡≤∏‡≤®‡≤º‡≤ä‡≤é‡≥ß‡≥É‡≥ä-'
+VOCABS['malayalam'] = '‡µ™‡¥â‡µÆ‡¥≥‡µµ‡¥î‡¥Ç‡¥∏‡¥û‡¥é‡¥∑‡µ´‡µÑ‡µå-‡¥É‡µà‡µÄ‡¥å‡¥õ‡¥á‡¥£‡¥æ‡¥à‡¥π‡¥ß‡µ≠‡¥ú‡¥ö‡µ±‡µ¥‡µπ‡¥Ø‡¥§‡µª‡¥∂‡¥í‡µØ‡¥ó‡µº‡¥ä‡¥Ü‡¥µ‡¥ñ‡µ†‡µ£‡µ©‡µã‡µΩ‡µß‡¥Ö‡µ≥‡µó‡¥™‡¥≠‡µÉ‡µç‡¥Æ‡µÜ‡¥ê‡µ°‡¥ì‡¥¶‡¥è‡¥±‡¥ø‡¥†‡¥∞‡µ∫‡µ∞‡µæ‡¥ô‡¥ü‡µ¶‡¥¢‡µ¢‡¥°‡¥≤‡µá‡¥¥‡¥ù‡µä‡µ≤‡¥¨‡¥®‡µÇ‡¥•‡µø‡¥ò‡¥´‡µÅ‡¥ã‡¥ï‡µ¨‡µ® '
+VOCABS['odia'] = '‡¨ñ‡≠Ø‡≠¨‡≠ã‡¨ì‡¨û‡≠ç‡¨∂‡≠™‡¨£‡¨•‡¨ö‡¨∞‡≠Ñ‡¨§‡¨É‡≠á‡≠Æ‡¨Ü‡¨ï‡¨µ‡≠Ç‡¨®‡¨¶‡≠∞‡≠ñ‡≠¢‡¨ú‡¨â‡¨≥‡¨Ö‡¨Å‡¨≤‡¨Ø‡¨î‡¨™‡≠≠‡¨∑‡¨¢‡≠ú‡¨ä‡≠ü‡¨Æ‡¨ø‡≠Å‡≠ß‡¨Ç‡¨º‡≠Ä‡¨¨‡¨ü‡¨≠‡≠ù‡≠¶‡¨ò‡¨†‡≠ó‡≠´‡≠°‡¨æ‡¨ê‡≠®‡¨ô‡¨π‡¨à‡≠±‡≠©‡≠É‡¨õ‡¨è‡≠å‡¨ó‡¨´‡¨∏‡¨á‡¨ß‡¨°‡¨ù‡≠à‡≠£‡≠†‡¨ã-‡•§'
+VOCABS['tamil'] = '‡ÆØ‡Ø¥‡Æ∑‡Ø´‡Øà‡ØÜ‡Æ∏‡Æé‡Æà‡Øã‡Æµ‡Ø≤‡ØÇ‡ØÅ‡Ø≠‡ÆÖ‡Øç‡Æ∂‡Æø‡Ø∞‡Æπ‡Øß‡Øê‡Ææ‡ØÆ‡Æî‡Ø∫‡Æö‡ØÄ‡Æ£‡Ø©‡Æá‡Æ©‡ÆÜ‡Æ¥‡Ø™‡ØØ‡Æô‡Æä‡Æ§‡Æú‡Ø∑‡Ø∂‡ÆÆ‡Øå‡Æ≥‡Ø∏‡Æê‡Æ™‡Æ®‡Øá‡Æ±‡Ø¨‡Æü‡Æí‡Øπ‡Æû‡Æâ‡Æè‡Æï‡Øó‡Øä‡Æ∞‡Ø±‡Øµ‡ÆÉ‡Ø®‡Æ≤‡Æì‡Ø≥‡Ø¶-'
+VOCABS['telugu'] = '‡±¶‡∞±‡∞ï‡∞Ü‡∞ã‡∞°‡∞§‡±Ø‡±ª‡∞ø‡∞π‡±å‡±≠‡±Ω‡∞â‡±Æ‡±ç‡∞ß‡∞ì‡∞ó‡±º‡∞Æ‡±´‡±Ç‡±†‡∞î‡∞æ‡∞á‡∞®‡±à‡∞Å‡∞ú‡±Ä‡±Ñ‡±Å‡±á‡∞∏‡∞∂‡±É‡∞É‡∞ù‡∞¢‡∞∞‡∞†‡∞≤‡±ã‡∞û‡±ò‡∞Ö‡±π‡±ß‡±¢‡∞õ‡∞¨‡±∏‡∞ê‡∞Ø‡±©‡∞ñ‡∞ü‡∞ö‡±Ü‡±ä‡∞ä‡∞¶‡∞à‡∞∑‡∞•‡∞≠‡∞è‡±ô‡±¨‡±æ‡∞é‡±™‡∞£‡∞í‡∞™‡±®‡∞´‡∞Ç‡∞ò‡∞ô‡∞≥‡∞µ‡±∫-'
+VOCABS['urdu'] = 'Ÿ±Ÿäÿ£€ÉÿØ€íÿ¥‚Äòÿ≤ÿπŸÉÿ¶⁄∫ÿ≥ÿ≠Ÿ∞ŸÜÿêÿ©ŸÇÿ∞ÿüÿî€î‚ÄîŸãŸÖ⁄æŸóŸæÿ∫Ÿñÿ∑ÿ•ÿíÿ±⁄ëÿµŸºŸç⁄Øÿßÿ§ÿ¨ÿ∂ŸíÔ∑∫⁄Ü‚Äé€ìŸêÿìŸëŸπÿ∏Ÿâÿ™⁄à‚Äç€åŸèŸáÿåÿÆŸàÿõÿ¢ŸÅÿ®ÿëŸÑ€Åÿ´Ô∫Ö‚Äå⁄òŸé€Çÿ°⁄©‚Äè'
+
+
+
+VOCABS['hindi'] = '‡•≤‡§Ω‡§ê‡§•‡§´‡§è‡§é‡§π‡•Æ‡••‡•â‡§Æ‡•Ø‡•Å‡§Å‡•ß‡§Ç‡•§‡§∑‡§ò‡§†‡§∞‡•ì‡•º‡•ú‡§ó‡§õ‡§ø‡•±‡§ü‡§©‡•Ñ‡§ë‡§µ‡§≤‡•´‡•ù‡•ü‡§Ö‡§û‡§∏‡§î‡§Ø‡§£‡•ë‡•ò‡•í‡•å‡•Ω‡§∂‡§ç‡•∞‡•Ç‡•Ä‡§í‡•ä‡•ô‡§â‡•õ‡•ª‡•Ö‡•©‡§ì‡§å‡§≥‡§®‡•†‡•¶‡•á‡§¢‡§ô‡•™‡§º‡•¢‡•ö‡§™‡§ä‡•ê‡§ú‡•®‡§°‡•à‡§≠‡§ù‡§ï‡§Ü‡§¶‡§¨‡§ã‡§ñ‡•æ‡•î‡•ã‡§á‡•ç‡§ß‡§§‡•û‡§à‡•É‡§É‡§æ‡•¨‡§ö‡§±‡§¥‡•≠-'
+VOCABS['sanskrit']='‡•õ‡§ã‡•Å‡•ú‡§ç‡§ê‡§ï‡•´‡§ü‡§Ø‡•™‡§â‡§É‡•©‡•†‡§ß‡•Ø‡•ç‡•≠‡•Ç‡•ß‡§µ‡§å‡•å‡•ê‡•°‡•¢‡§á‡•¨‡§æ‡•à‡•Æ‡§®‡•É‡§Ö‡§Ç‡§•‡§¢‡•á‡§ñ‡§î‡§ò‡•ö‡•¶‡§≤‡§ú‡•ã‡§à‡§∞‡§û‡§™‡§´‡§Å‡§ù‡§≠‡§∑‡•Ö‡•Ñ‡§ó‡§§‡§ö‡§π‡§∏‡•Ä‡•ù‡§Ü‡§∂‡§è‡•§‡§Æ‡•®‡§¶‡§†‡§ô‡§¨‡§ø‡§ä‡§°‡§ì‡§≥‡§õ‡§£‡§º‡§Ω'
+VOCABS['devanagari'] = '‡§∞‡§ö‡•ô‡•©‡•æ‡§ç‡•É‡•á‡§û‡§≤‡•ª‡•â‡§¥‡§∑‡•ê‡•¢‡•ß‡§Ø‡•¶‡•Ω‡§è‡§æ‡•®‡§à‡•§‡•ö‡•≠‡§ü‡§ê‡•ü‡••‡§§‡•ã‡§¶‡§Ω‡§≠‡•Å‡§®‡§ì‡§í-‡§†‡§Å.‡•å‡•ç‡•Æ‡•º‡§ù‡•†‡§µ‡§ø‡§É‡•ò‡•Ä‡•∞‡§õ‡•Ö‡•ä‡§©‡§±‡§º‡§•‡§ú‡§∂‡§≥‡§ô‡§Ö‡§ã‡§ñ‡§¨‡§´‡§â‡•´‡•û‡•¨‡§ä‡•≤‡•Ü‡•õ‡§ï‡•ù‡§Æ‡•Ç‡§∏‡•ì‡§á‡§î‡§π‡•ë‡•à‡§ó‡§¢‡•£‡§ß‡§Ü‡•ú‡•Ø‡§Ç‡•™‡§°‡§£‡§™‡•Ñ‡§ò‡§ë'
+VOCABS["latin"] = (
+    VOCABS["digits"] 
+    + VOCABS["ascii_letters"] 
+    + VOCABS["punctuation"]
+    )
 VOCABS["english"] = VOCABS["latin"] + "¬∞" + VOCABS["currency"]
-VOCABS["legacy_french"] = VOCABS["latin"] + "¬∞" + "√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√ß√Ä√Ç√â√à√ã√é√è√î√ô√õ√á" + VOCABS["currency"]
-VOCABS["french"] = VOCABS["english"] + "√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√º√ß√Ä√Ç√â√à√ä√ã√é√è√î√ô√õ√ú√á"
+VOCABS["legacy_french"] = VOCABS["latin"] + "¬∞" + "√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√ß√Ä√Ç√â√à√ã√é√è√î√ô√õ√á" + 'oQ‡•ê‚àó}Ã≠‚â•NÃÉ‡¢áu‚Äì‡§æ1\'¬∞9≈õ‡¶æ‡•®+h‡§ú‡¶∑‡§Ö‡•£‚àòVÃ§‡§îw`(‡§§≈´$‡§ã‡§ê‡•å‡§∑‡§ö‡§©‡•©C‡§ò[‡¶øk‚Äù‡§õ‡§™s‚àìa‡•á‚Ä¶‚Äç‡§âOe‡§Ωl)‡•ü‚ä§‡§•·∏π‚åä‡¶∞‡•Ø‡•ß‡§†‡§±‡¶™‡¶ü·π£·πÉ:g‚Äúq!y‡¶≤‡§Ø~‡§≤|‚óåpÃÇ]PT‡¶ï‡§∏z&U‡•à·πù‡•ä‚Äî·∫è‡§¨‡¶ñ·πõ‡§≥‡§Ü√ó‡§Å‡•∞‚Äô‡•Ä‡•ëKd‡§ó‡•¶‡•ú‡•ÑZ‡•¨F‚àíƒ°‡§π·πü4{‡§¶Ãá‡§å‡•°‚â§‚ãÜS·≥≥‚à¥\b‡§ñ‡¶¨‡•Ö‡•ç‚åã.>‡§ù‡•ù0√≤‡••·π≠‚àµ‚àø‡§èƒ´‡¶É8?‡§µ‡•Å‡§ô-I‡§øBi‡¶∂rnmÕü‡•õH‡•ÉW2xY‡¶®%‡•≠‡§ü6X‡•â‡§¢/‡§≠‚Äë=f‡§É‡§ß·πá·πÖ‚ÄåA‚àºRv‡§û‡•™7‡•†5√®‡ßÅ‡¶Æ‚àæM·∏ç‡§∂‡§á‡•ÆDùúÅ‡§é√∑,‡•ã‡§£;‡§Æ‡•Ç‚Ä†L‡¶∏3‡§ï‡§°·∏•j‡•¢E√±‚àô‚àª‚àΩGJ‡ßÄ‡•´‡§´·πâcƒÅ*‚Äò‡§º‚àÆ·∏∑"‡§∞‡§ä<‡ßç‡•í‡§®_‚àöt‡§à‡•§‚à†‡§ì' + VOCABS["currency"] #+ VOCABS["sanskrit_transliterated"]
+VOCABS["french"] = VOCABS["english"] + "'(-.0123456789:A[abcdeghijklmnoprstuvy|√±ƒÅƒìƒ´≈ç≈õ≈´ÃÑÃêÃ•‡•§‡••·∏ç·∏•·πÅ·πÉ·πÖ·πá·πõ·π£·π≠\u200c\u200d" #"√†√¢√©√®√™√´√Æ√Ø√¥√π√ª√º√ß√Ä√Ç√â√à√ä√ã√é√è√î√ô√õ√ú√á"
 VOCABS["portuguese"] = VOCABS["english"] + "√°√†√¢√£√©√™√≠√Ø√≥√¥√µ√∫√º√ß√Å√Ä√Ç√É√â√ä√ç√è√ì√î√ï√ö√ú√á"
 VOCABS["spanish"] = VOCABS["english"] + "√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë" + "¬°¬ø"
 VOCABS["german"] = VOCABS["english"] + "√§√∂√º√ü√Ñ√ñ√ú·∫û"
@@ -44,3 +69,21 @@ VOCABS["vietnamese"] = (
     + "√°√†·∫£·∫°√£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªô·ªó∆°·ªõ·ªù·ªü·ª£·ª°√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±i√≠√¨·ªâƒ©·ªã√Ω·ª≥·ª∑·ªπ·ªµ"
     + "√Å√Ä·∫¢·∫†√ÉƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªò·ªñ∆†·ªö·ªú·ªû·ª¢·ª†√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞I√ç√å·ªàƒ®·ªä√ù·ª≤·ª∂·ª∏·ª¥"
 )
+VOCABS["transliterated"] = (
+    VOCABS["english"]
+    + VOCABS["sanskrit_transliterated"]
+    )
+
+
+VOCABS["diacritics_training"] = (
+    VOCABS["english"]
+     + VOCABS["sanskrit_transliterated"]
+    #+VOCABS["devanagari"]
+     #  +VOCABS["sanskrit_unicode"]
+
+    )
+
+#print("\n\n\n")
+print(VOCABS["diacritics_training"])
+#print("\n\n\n")
+
diff --git a/doctr/models/recognition/crnn/pytorch.py b/doctr/models/recognition/crnn/pytorch.py
index b1e50f1..8aaafac 100644
--- a/doctr/models/recognition/crnn/pytorch.py
+++ b/doctr/models/recognition/crnn/pytorch.py
@@ -17,9 +17,17 @@ from ...classification import mobilenet_v3_large_r, mobilenet_v3_small_r, vgg16_
 from ...utils.pytorch import load_pretrained_params
 from ..core import RecognitionModel, RecognitionPostProcessor
 
-__all__ = ["CRNN", "crnn_vgg16_bn", "crnn_mobilenet_v3_small", "crnn_mobilenet_v3_large"]
+__all__ = ["CRNN","crnn_vgg16_bn_diacritics", "crnn_vgg16_bn", "crnn_mobilenet_v3_small", "crnn_mobilenet_v3_large"]
 
 default_cfgs: Dict[str, Dict[str, Any]] = {
+    "crnn_vgg16_bn_diacritics": {
+        "mean": (0.694, 0.695, 0.693),
+        "std": (0.299, 0.296, 0.301),
+        "input_shape": (3, 32, 128),
+        "vocab": VOCABS["diacritics_training"],
+        #"url": "https://doctr-static.mindee.com/models?id=v0.3.1/crnn_vgg16_bn-9762b0b0.pt&src=0",
+        "url":"https://github.com/navaneeth031/navindicDocTRforSansDiacritics/releases/download/Model-IASTENG/crnn_vgg16_bn_diacritics_20230815-145907_0.00003_IASTand.Eng_v1.pt",
+    },
     "crnn_vgg16_bn": {
         "mean": (0.694, 0.695, 0.693),
         "std": (0.299, 0.296, 0.301),
@@ -330,3 +338,7 @@ def crnn_mobilenet_v3_large(pretrained: bool = False, **kwargs: Any) -> CRNN:
         ignore_keys=["linear.weight", "linear.bias"],
         **kwargs,
     )
+
+def crnn_vgg16_bn_diacritics(pretrained: bool = False, **kwargs: Any) -> CRNN:
+    #pretrained_backbone=False -- add this for training 
+    return _crnn("crnn_vgg16_bn_diacritics", pretrained, vgg16_bn_r, ignore_keys=["linear.weight", "linear.bias"], **kwargs)
\ No newline at end of file
diff --git a/doctr/models/recognition/zoo.py b/doctr/models/recognition/zoo.py
index 74f5c40..c87c300 100644
--- a/doctr/models/recognition/zoo.py
+++ b/doctr/models/recognition/zoo.py
@@ -15,6 +15,7 @@ __all__ = ["recognition_predictor"]
 
 
 ARCHS: List[str] = [
+    "crnn_vgg16_bn_diacritics",
     "crnn_vgg16_bn",
     "crnn_mobilenet_v3_small",
     "crnn_mobilenet_v3_large",
diff --git a/references/recognition/README.md b/references/recognition/README.md
index 5fa5510..57e41fd 100644
--- a/references/recognition/README.md
+++ b/references/recognition/README.md
@@ -97,3 +97,14 @@ Running the training script should look like this for multiple custom fonts:
 ```shell
 python references/recognition/train_pytorch.py crnn_vgg16_bn --epochs 5 --font "custom-font-1.ttf,custom-font-2.ttf"
 ```
+
+##########################################################################
+Local guidelines pertaining to the current workspace.
+
+Training command (working as of 10,aug,23):
+
+python references/recognition/train_pytorch.py crnn_vgg16_bn_diacritics --epochs 2 --device 1 --font DejaVuSans-Bold.ttf -b 512 --vocab diacritics_training --input_size 64
+
+Training command (training completed --crnn_vgg16_bn_diacritics_20230811-020435-200e.pt):
+
+python references/recognition/train_pytorch.py crnn_vgg16_bn_diacritics --epochs 200 --device 1 --font /home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Arial.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Cardo104s.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/DejaVuSans.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/EBGaramond12-Regular.otf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Georgia.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Lato-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Quicksand-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Times_New_Roman.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/eng/Verdana.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/iast/Carlito-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/iast/CharisSIL-R.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/iast/NotoSans-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/iast/OpenSans-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/chandas.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/Lohit-Devanagari.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/NotoSansDevanagari-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/Poppins-Regular.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/Sanskrit2003.ttf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/Shobhika-Regular.otf,/home/venkat/workspace/sanskritdiacritics-doctr/fontsForDiacritics/devanagari/uttara.ttf -b 1024 --vocab diacritics_training --input_size 32
diff --git a/references/recognition/train_pytorch.py b/references/recognition/train_pytorch.py
index c419571..c2a5144 100644
--- a/references/recognition/train_pytorch.py
+++ b/references/recognition/train_pytorch.py
@@ -12,6 +12,7 @@ import hashlib
 import logging
 import multiprocessing as mp
 import time
+import json
 from pathlib import Path
 
 import numpy as np
@@ -151,6 +152,8 @@ def evaluate(model, val_loader, batch_transforms, val_metric, amp=False):
     val_metric.reset()
     # Validation loop
     val_loss, batch_cnt = 0, 0
+    predictionsForSavingResult = []
+
     for images, targets in val_loader:
         if torch.cuda.is_available():
             images = images.cuda()
@@ -160,6 +163,19 @@ def evaluate(model, val_loader, batch_transforms, val_metric, amp=False):
                 out = model(images, targets, return_preds=True)
         else:
             out = model(images, targets, return_preds=True)
+
+        #print("\n\nout: ",out["loss"])
+        #print("\n\nlen(targets): ",targets[0:10])
+
+        #break
+
+        d = {}
+        for eleIndex in range(0,len(out['preds'])):
+            d['pred'] = out['preds'][eleIndex]
+            d['actual'] = targets[eleIndex]
+
+            predictionsForSavingResult.append(d)
+        
         # Compute metric
         if len(out["preds"]):
             words, _ = zip(*out["preds"])
@@ -170,6 +186,10 @@ def evaluate(model, val_loader, batch_transforms, val_metric, amp=False):
         val_loss += out["loss"].item()
         batch_cnt += 1
 
+    print("length of predictionsForSavingResult",len(predictionsForSavingResult))
+    with open("/home/venkat/workspace/sanskritdiacritics-doctr/data/data/results.json", "w") as myfinalresultfile:
+        json.dump(predictionsForSavingResult, myfinalresultfile, indent=4, ensure_ascii=False)
+
     val_loss /= batch_cnt
     result = val_metric.summary()
     return val_loss, result["raw"], result["unicase"]
@@ -208,7 +228,9 @@ def main(args):
             min_chars=args.min_chars,
             max_chars=args.max_chars,
             num_samples=args.val_samples * len(vocab),
+            words_txt_path=args.words_txt_path,
             font_family=fonts,
+            save_samples_root = args.save_samples_root,
             img_transforms=Compose(
                 [
                     T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),
@@ -307,7 +329,9 @@ def main(args):
             min_chars=args.min_chars,
             max_chars=args.max_chars,
             num_samples=args.train_samples * len(vocab),
+            words_txt_path=args.words_txt_path,
             font_family=fonts,
+            save_samples_root=None,
             img_transforms=Compose(
                 [
                     T.Resize((args.input_size, 4 * args.input_size), preserve_aspect_ratio=True),
@@ -460,6 +484,7 @@ def parse_args():
     parser.add_argument(
         "--show-samples", dest="show_samples", action="store_true", help="Display unormalized training samples"
     )
+    parser.add_argument("--words_txt_path", help="The text file contains the words to prepare dataset from.")
     parser.add_argument("--wb", dest="wb", action="store_true", help="Log to Weights & Biases")
     parser.add_argument("--push-to-hub", dest="push_to_hub", action="store_true", help="Push to Huggingface Hub")
     parser.add_argument(
@@ -471,6 +496,8 @@ def parse_args():
     parser.add_argument("--sched", type=str, default="cosine", help="scheduler to use")
     parser.add_argument("--amp", dest="amp", help="Use Automatic Mixed Precision", action="store_true")
     parser.add_argument("--find-lr", action="store_true", help="Gridsearch the optimal LR")
+    parser.add_argument("--save_samples_root", help="saves samples as images")
+
     args = parser.parse_args()
 
     return args
